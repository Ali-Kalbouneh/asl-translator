# âœ‹ ASL Translator

This project is **not meant to be a serious ASL translation tool**.  
It's simply a **personal project** where I experiment with:

- MediaPipe (hand tracking)  
- OpenCV (webcam + visualization)  
- Basic machine learning (RandomForest)  

The goal is to learn and explore â€” not to build a production-grade ASL interpreter.

---

## ğŸ“ Project Structure

```
asl-translator/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ letters.csv             # Collected samples (A, B, C)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ asl_letters_rf.pkl      # Trained RandomForest model
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection.py      # Collects training data
â”‚   â”œâ”€â”€ train_model.py          # Trains the model
â”‚   â”œâ”€â”€ interpreter.py          # Real-time ASL interpreter
â”‚   â”œâ”€â”€ hand_tracker.py         # MediaPipe hand detection wrapper
â”‚   â”œâ”€â”€ features.py             # Feature extraction + normalization
â”‚   â””â”€â”€ show_landmarks.py       # Debugging utility
â”‚
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

Clone the repository:

```bash
git clone https://github.com/<your-username>/asl-translator.git
cd asl-translator
```

Install dependencies:

```bash
pip install opencv-python mediapipe numpy scikit-learn joblib
```

---

## ğŸ— Collecting Training Data (A, B, C)

Run the data collection script:

```bash
python3 src/data_collection.py
```

### Controls

- **A**, **B**, **C** â†’ select which letter to record  
- **3** â†’ save the current frame as a sample  
- **ESC** â†’ exit  

Samples are saved to:

```
data/letters.csv
```

### Tips for best results

- Collect **60â€“120 samples per letter**  
- Use small variations in angle, distance, lighting  
- Keep your hand steady when saving  
- Ensure your hand is fully inside the frame  

---

## ğŸ“ Training the Model

Once you have collected enough samples, train the model:

```bash
python3 src/train_model.py
```

This will:

- Load all data  
- Train a RandomForest classifier  
- Print accuracy and a classification report  
- Save the trained model to:

```
models/asl_letters_rf.pkl
```

---

## ğŸ¤– Running the Real-Time Interpreter

After training the model:

```bash
python3 src/interpreter.py
```

The interpreter:

- Opens your webcam  
- Uses MediaPipe to detect hand landmarks  
- Extracts normalized features  
- Predicts A/B/C in real time  
- Displays the prediction on screen  
- Draws the hand skeleton  
- Smooths predictions to reduce flicker  

---

## ğŸ§ª Notes & Limitations

This is just a fun experiment. Current limitations:

- Only supports **A, B, C**  
- Sensitive to hand rotation and lighting  
- No â€œrest hand / noneâ€ class yet  
- Flickers occasionally  
- Not meant for actual ASL communication  

Still, itâ€™s a great way to learn:

- Computer vision basics  
- MediaPipe landmark extraction  
- Feature engineering  
- Machine learning model training  
- Real-time CV application design  

---

## ğŸš§ Future Ideas

- Add a **NONE** / rest-hand class  
- Add more letters (Aâ€“Z)  
- Recognize dynamic gestures (J, Z)  
- Add a sentence builder  
- Use a neural-network-based model  
- Support two-hand signs  

---

