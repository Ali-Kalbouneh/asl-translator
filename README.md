âœ‹ ASL Translator
This is not meant to be a research-grade ASL interpreter or anything serious.
It's simply a fun personal project where I experiment with computer vision, MediaPipe, and basic machine learning to recognize a few static ASL hand shapes.

The goal is to learn, explore, and gradually improve â€” not to create a full ASL translation system.

For now, I have only trained the model on (A, B, C), and will train it further as time passes.

ğŸ“ Project Structure:
asl-translator/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ letters.csv             # All collected samples (A, B, C)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ asl_letters_rf.pkl      # Trained RandomForest model
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection.py      # Script for collecting training data
â”‚   â”œâ”€â”€ train_model.py          # Trains the ML model
â”‚   â”œâ”€â”€ interpreter.py          # Real-time ASL interpreter
â”‚   â”œâ”€â”€ hand_tracker.py         # Mediapipe hand detection wrapper
â”‚   â”œâ”€â”€ features.py             # Normalization / feature extraction
â”‚   â””â”€â”€ show_landmarks.py       # Debugging tool to visualize landmarks
â”‚
â””â”€â”€ README.md


âš™ï¸ Setup & Installation:
Clone the repo:
git clone https://github.com/<Ali-Kalbouneh>/asl-translator.git
cd asl-translator

Install Dependencies:
pip install opencv-python mediapipe numpy scikit-learn joblib


ğŸ— Collecting Training Data:

Before training the model(if you want to), you need to collect examples of your hand forming 

ASL letters.

Run:

python3 src/data_collection.py

Controls:

Press A, B, or C â†’ choose a letter to record

Press 3 â†’ save a sample of your hand

Press ESC â†’ exit the tool

All samples are stored in: data/letters.csv


ğŸ“ Training the Model:

After collecting enough data run:

python3 src/train_model.py

This will:

Load your dataset

Train a RandomForest classifier

Print training and test accuracy

Save the trained model to: models/asl_letters_rf.pkl


ğŸ¤– Running the Real-Time Interpreter

Once the model is trained, run: 

python3 src/interpreter.py

The interpreter:

Opens your webcam

Detects your hand using MediaPipe

Extracts normalized landmark features

Predicts A/B/C in real time

Displays the predicted letter on-screen

Draws hand landmarks for debugging

Uses smoothing to reduce flicker

Hold up the ASL letters and watch it attempt to classify them.



ğŸ§ª Notes & Limitations

This is just a fun experiment, not an actual ASL translation tool.

Current limitations:

Only supports A, B, and C

Sensitive to hand rotation, lighting, and camera distance

Occasional misclassifications

No dynamic signs (J, Z)

Not meant for real communication

But itâ€™s a great learning project for:

Computer vision

Hand-tracking

Feature engineering

Machine learning

Real-time interactive systems


ğŸš§ Future Ideas

Add a NONE / rest class

Expand dataset to full alphabet (Aâ€“Z)

Add dynamic gestures (J, Z)

Use a deep learning model (CNN/MLP)

Add sentence builder / UI overlay

Support two-hand signs
